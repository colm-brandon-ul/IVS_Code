{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from website_parser import get_website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from website_parser.information_extraction import information_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.thebrighterside.news/post/innovation-generates-1-000x-more-voltage-from-solar-cells'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gW = get_website.Get_Website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gW.get_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status_code', 'content'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = information_extraction.Information_Extraction(\n",
    "    url = res['content']['url'],\n",
    "    html = res\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iE.extract_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_pre_processing.tokenization as tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = tokenization.Tokenization(sentence_map=data['content']['natural_language_data']['content']['sentence_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkns = tkn.tokenize_sentence_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_pre_processing.token_cleaning as token_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkCln = token_cleaning.Token_Cleaning(token_map=tkns['content']['token_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = tkCln.remove_punctutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status_code', 'content'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': {2: {'token': '1,000x',\n",
       "   'type_of_edit': 'removal_of_intra_word_punctuation'}},\n",
       " '1.0': {0: {'token': '[',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  2: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  3: {'token': ':', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  5: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  7: {'token': ']', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '2.0': {7: {'token': ';',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  8: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  12: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '2.1': {0: {'token': '(',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  1: {'token': ':', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  3: {'token': ')', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '3.0': {13: {'token': '1,000',\n",
       "   'type_of_edit': 'removal_of_intra_word_punctuation'},\n",
       "  24: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '3.1': {14: {'token': '(',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  15: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '3.2': {10: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  24: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '3.3': {2: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  11: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  20: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '4.0': {7: {'token': ';',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  8: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  12: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '4.1': {8: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  14: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  22: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '4.2': {13: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  28: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '4.3': {17: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '4.4': {2: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  16: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  19: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  25: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '4.5': {10: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '5.0': {1: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  16: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '5.1': {19: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '5.2': {16: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '5.3': {8: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  15: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  28: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  31: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '6.0': {21: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  27: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '6.1': {2: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  13: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  14: {'token': ':', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  26: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '6.2': {17: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '6.3': {14: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '10.0': {2: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '11.0': {4: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  10: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '11.1': {0: {'token': '(',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  1: {'token': ':', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  3: {'token': '/', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  5: {'token': ')', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '12.0': {5: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  13: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '12.1': {7: {'token': ':',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  16: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  22: {'token': '1,000', 'type_of_edit': 'removal_of_intra_word_punctuation'},\n",
       "  48: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '13.0': {19: {'token': ',',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  36: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  40: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '13.1': {10: {'token': ':',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  18: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '14.0': {16: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '14.1': {20: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '14.2': {15: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '14.3': {13: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '17.0': {0: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '18.0': {1: {'token': ':',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  7: {'token': '.', 'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '18.1': {8: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '19.0': {7: {'token': '?',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '21.0': {0: {'token': '.',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'}},\n",
       " '22.0': {1: {'token': ':',\n",
       "   'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  3: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  5: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  7: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  9: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  11: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  13: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'},\n",
       "  15: {'token': ',', 'type_of_edit': 'removal_of_inter_word_punctuation'}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens['content']['edit_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.3.16',\n",
       " '3.3.19',\n",
       " '4.2.0',\n",
       " '4.2.13',\n",
       " '4.3.0',\n",
       " '4.3.17',\n",
       " '5.2.0',\n",
       " '5.3.28',\n",
       " '6.1.14',\n",
       " '6.3.14',\n",
       " '13.0.0',\n",
       " '13.0.36',\n",
       " '14.2.0',\n",
       " '14.3.13',\n",
       " '20.0.4']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens['content']['quote_identifiers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrn = re.compile('^[0-9]*.[0-9]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_indexes(quote_index_string):\n",
    "    #Takes int he Quote identifier which is a string in format\n",
    "    # 'paragraph_index.sentence_index.token_index'\n",
    "    #Return a tuple of string 'paragraph_index.sentence_index' and int token_index\n",
    "    par_sent = re.findall(ptrn,quote_index_string)[0]\n",
    "    token_no = quote_index_string.replace('{}.'.format(par_sent), '')\n",
    "    return par_sent, int(token_no)\n",
    "\n",
    "\n",
    "def get_par_sentence_displace(open, close):\n",
    "    open_par, open_sent = [int(x) for x in open.split('.')]\n",
    "    close_par, close_sent = [int(x) for x in close.split('.')]\n",
    "    par_dis = close_par - open_par\n",
    "    sent_dis = close_sent - open_sent\n",
    "\n",
    "    return par_dis, sent_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = list(grouper(clean_tokens['content']['quote_identifiers'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.3.16',\n",
       " '3.3.19',\n",
       " '4.2.0',\n",
       " '4.2.13',\n",
       " '4.3.0',\n",
       " '4.3.17',\n",
       " '5.2.0',\n",
       " '5.3.28',\n",
       " '6.1.14',\n",
       " '6.3.14',\n",
       " '13.0.0',\n",
       " '13.0.36',\n",
       " '14.2.0',\n",
       " '14.3.13',\n",
       " '20.0.4']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens['content']['quote_identifiers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'Science', 'Advances', \"''\"]\n",
      "\n",
      "['``', 'Ferroelectric', 'means', 'that', 'the', 'material', 'has', 'spatially', 'separated', 'positive', 'and', 'negative', 'charges', \"''\"]\n",
      "\n",
      "['``', 'The', 'charge', 'separation', 'leads', 'to', 'an', 'asymmetric', 'structure', 'that', 'enables', 'electricity', 'to', 'be', 'generated', 'from', 'light', \"''\"]\n",
      "\n",
      "['``', 'The', 'interaction', 'between', 'the', 'lattice', 'layers', 'appears', 'to', 'lead', 'to', 'a', 'much', 'higher', 'permittivity', '-', 'in', 'other', 'words', 'the', 'electrons', 'are', 'able', 'to', 'flow', 'much', 'more', 'easily', 'due', 'to', 'the', 'excitation', 'by', 'the', 'light', 'photons', \"''\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for blcks in blocks:\n",
    "    if blcks[0] != None and blcks[1] != None:\n",
    "        open_parsent, open_token = extract_indexes(blcks[0])\n",
    "        close_parsent, close_token = extract_indexes(blcks[1])\n",
    "        #print(blcks)\n",
    "        d_par, d_sent = (get_par_sentence_displace(open_parsent,close_parsent))\n",
    "        #Single sentence\n",
    "        if d_par == 0 and d_sent == 0:\n",
    "            sentence = clean_tokens['content']['clean_token_map'][open_parsent]\n",
    "            #print(blcks)\n",
    "            #print(sentence)\n",
    "            print(sentence[open_token:close_token +1])\n",
    "            print()\n",
    "        #Multi sentence\n",
    "        elif d_par == 0 and d_sent > 0:\n",
    "            pass\n",
    "        # Multi paragraph\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_parsent\n",
    "close_parsent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['Their', 'findings', 'which', 'could', 'significantly', 'increase', 'the', 'efficiency', 'of', 'solar', 'cells', 'were', 'published', 'in', 'the', 'journal', '``', 'Science', 'Advances', \"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Science', 'Advances']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
